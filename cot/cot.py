import os
import time
import re
import argparse
import random
import gc
import torch
from tqdm import tqdm

vars_to_remove = [
    'HF_HUB_USER_AGENT', 'HUGGING_FACE_HUB_TOKEN', 'HF_TOKEN',
    'HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy', 
    'ALL_PROXY', 'all_proxy'
]
for var in vars_to_remove:
    if var in os.environ:
        del os.environ[var]

from huggingface_hub import login
login(token="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")

from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import json
from utils.retrieve_similar_examples import retrieve_topk
from utils.rank import rank_examples_by_sparsity, analyze_difficulty_levels, get_difficulty_level

# å¯¼å…¥æ•°å­¦ç­”æ¡ˆç­‰ä»·æ€§åˆ¤æ–­æ¨¡å—
from math_equivalence import get_answer, is_equiv

def parse_args():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ vLLM è§£å†³ MATH-500 é—®é¢˜")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="æ¨¡å‹è·¯å¾„æˆ–åç§°ï¼ˆæœ¬åœ° vLLM æ¨¡å‹ï¼‰")
    parser.add_argument("--gpu_id", type=int, default=2,
                        help="GPU ç¼–å·")
    parser.add_argument("--test_ratio", type=float, default=0.8, help="æµ‹è¯•é›†æ¯”ä¾‹")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.5,
                        help="GPU æ˜¾å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--use_cot", type=str, default='curriculum', choices=['cot', 'few-shot', 'curriculum', 'auto-shot'],
                        help="æ˜¯å¦ä½¿ç”¨ Chain-of-Thought prompting")
    parser.add_argument("--num_few_shot", type=int, default=2,
                        help="Few-shot ç¤ºä¾‹æ•°é‡")
    parser.add_argument("--example_pool_size", type=int, default=500,
                        help="ç¤ºä¾‹æ± å¤§å°ï¼ˆä»è®­ç»ƒé›†ä¸­é€‰æ‹©çš„æ ·æœ¬æ•°é‡ï¼‰")
    parser.add_argument("--verbose", action="store_true", default=False,
                        help="æ˜¯å¦æ‰“å°æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†å›å¤")
    parser.add_argument("--print_freq", type=int, default=100,
                        help="æ‰“å°è¯¦ç»†ä¿¡æ¯çš„é¢‘ç‡ï¼ˆæ¯Nä¸ªæ ·æœ¬ï¼‰")
    parser.add_argument("--rank_metric", type=str, default="l0_norm",
                        choices=["l0_norm", "top10pct_ratio", "effective_rank"],
                        help="ç¤ºä¾‹æ± æ’åºæŒ‡æ ‡ï¼ˆç”¨äº curriculum learningï¼‰")
    parser.add_argument("--dataset", type=str, default="math500",
                        choices=["math500"])
    parser.add_argument("--use_local_dataset", action="store_true", default=True,
                        help="ä½¿ç”¨æœ¬åœ° Math-500 æ•°æ®é›†ï¼ˆ./dataset/Math-500/ï¼‰")
    parser.add_argument("--local_dataset_path", type=str, default="./dataset/Math-500",
                        help="æœ¬åœ°æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­ï¼Œç”¨äºå›ºå®š few-shot æ ·æœ¬é€‰æ‹©ï¼Œç¡®ä¿å®éªŒå¯å¤ç°")
    parser.add_argument("--max_tokens", type=int, default=4096,
                        help="ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡")
    parser.add_argument("--sample_test_size", type=int, default=None,
                        help="ä»æµ‹è¯•é›†ä¸­éšæœºæŠ½æ ·çš„æ ·æœ¬æ•°é‡ï¼ˆNone è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†ï¼‰")
    parser.add_argument("--n_levels", type=int, default=5,
                        help="éš¾åº¦ç­‰çº§æ•°é‡ï¼ˆç”¨äº curriculum learningï¼‰")
    return parser.parse_args()





def prepare_prompt_math500(item, tokenizer, use_cot='zero-shot', example_pool=None, num_few_shot=1, dataset_type='math500', 
                           model_path=None, rank_metric=None, return_messages=False, difficulty_thresholds=None):
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©æ­£ç¡®çš„å­—æ®µ
   
    problem = item.get('problem', item.get('question', ''))
    ground_truth = item.get('answer', item.get('solution', ''))
    level = item.get('level', 'N/A')

    if use_cot == 'cot':
        messages = [
            {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."},
            {"role": "user", "content": f"{problem}\n\n Please solve this step by step and provide your final answer."}
        ]
    if use_cot == 'auto-shot':
        num_auto_shot = num_few_shot
        messages = [
            {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."}
        ]
        
        similar_examples = retrieve_topk(problem, example_pool, top_k=num_auto_shot)
        for score, rank_idx, example in similar_examples:
            print(score, rank_idx, example)
            messages.append({"role": "user", "content": f"{example.get('problem', '')}\n\nPlease solve this step by step and provide your final answer."})
            messages.append({"role": "assistant", "content": example.get('solution', '')})
        messages.append({"role": "user", "content": f"{problem}\n\nPlease solve this step by step and provide your final answer."})
    
    elif use_cot == 'few-shot':
       
        messages = [
            {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."}
        ]
       
        num_examples = min(num_few_shot, len(example_pool))
        random_indices = random.sample(range(len(example_pool)), num_examples)
        
        for idx in random_indices:
            random_example = example_pool[idx]
            question_example = random_example.get("problem", "")
            answer_example = random_example.get("solution", "")
            messages.append({"role": "user", "content": f"{question_example}\n\nPlease solve this step by step and provide your final answer."})
            messages.append({"role": "assistant", "content": answer_example})
        
        # æ·»åŠ å®é™…é—®é¢˜
        messages.append({"role": "user", "content": f"{problem}\n\nPlease solve this step by step and provide your final answer."})
        
    elif use_cot == 'curriculum':
        sparsity_score = item.get('sparsity_score', 0)
        difficulty_level, difficulty_level_name = get_difficulty_level(sparsity_score, difficulty_thresholds)
        
        top_k = min(20, len(example_pool))
        similar_examples = retrieve_topk(problem, example_pool, top_k=top_k)
        
        max_level = max(difficulty_thresholds.keys())
        
        # è‡ªåŠ¨æ ¹æ® max_level åˆ’åˆ†ä¸‰ä¸ªéš¾åº¦åŒºé—´
        easy_threshold = max_level // 3                    # ç®€å•åŒºé—´ä¸Šé™
        medium_threshold = (max_level * 2) // 3            # ä¸­ç­‰åŒºé—´ä¸Šé™
        # ç®€å•é—®é¢˜ï¼šdifficulty_level <= easy_threshold
        # ä¸­ç­‰é—®é¢˜ï¼šeasy_threshold < difficulty_level <= medium_threshold
        # éš¾é—®é¢˜ï¼šdifficulty_level > medium_threshold
        
        if difficulty_level < easy_threshold:
            # ç®€å•é—®é¢˜ï¼šåªéœ€è¦ 1 ä¸ªåŒçº§åˆ«çš„ç›¸ä¼¼ä¾‹å­ å’Œä¸€ä¸ªæ›´é«˜çº§çš„ç›¸ä¼¼ä¾‹å­
            question1, answer1 = '', ''
            for score, rank_idx, example in similar_examples:
                if example.get('difficulty_level', 0) == difficulty_level:
                    question1 = example.get('problem', '')
                    answer1 = example.get('solution', '')
                    break
            question2, answer2 = '', ''
            for score, rank_idx, example in similar_examples:
                if example.get('difficulty_level', 0) == difficulty_level + 1:
                    question2 = example.get('problem', '')
                    answer2 = example.get('solution', '')
                    break
            
            
            messages = [
                {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."},
                {"role": "user", "content": f"{question1}\n\nPlease solve this problem step by step and provide your final answer."},
                {"role": "assistant", "content": answer1},
                {"role": "user", "content": f"{question2}\n\nPlease solve this problem step by step and provide your final answer."},
                {"role": "assistant", "content": answer2},
                {"role": "user", "content": f"{problem}\n\nPlease solve this problem step by step and provide your final answer."},
            ]
        
        elif difficulty_level < medium_threshold:
            # ä¸­ç­‰é—®é¢˜ï¼š1 ä¸ªåŒçº§åˆ«çš„ç›¸ä¼¼ä¾‹å­ å’Œä¸€ä¸ªæ›´é«˜çº§çš„ç›¸ä¼¼ä¾‹å­
            examples_found = []
            for score, rank_idx, example in similar_examples:
                if example.get('difficulty_level', 0) == max(1, difficulty_level):
                    examples_found.append(example)
                    if len(examples_found) >= 2:
                        break
            # è¡¥è¶³åˆ° 2 ä¸ª
            if len(examples_found) < 2:
                for score, rank_idx, example in similar_examples:
                    if example not in examples_found :
                        examples_found.append(example)
                        if len(examples_found) >= 2:
                            break
            
            messages = [
                {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."},
            ]
            for ex in examples_found:
                messages.append({"role": "user", "content": f"{ex.get('problem', '')}\n\nPlease solve this problem step by step and provide your final answer."})
                messages.append({"role": "assistant", "content": ex.get('solution', '')})
            messages.append({"role": "user", "content": f"{problem}\n\nPlease solve this problem step by step and provide your final answer."})
        
        else:
            # éš¾é—®é¢˜ (level 4-5)ï¼šä½ä¸€çº§ + åŒçº§åˆ«ï¼Œå¾ªåºæ¸è¿›
            lower_level = max(1, difficulty_level - 1)
            
            question1, answer1 = '', ''
            for score, rank_idx, example in similar_examples:
                if example.get('difficulty_level', 0) == lower_level:
                    question1 = example.get('problem', '')
                    answer1 = example.get('solution', '')
                    break
            
            question2, answer2 = '', ''
            for score, rank_idx, example in similar_examples:
                if example.get('difficulty_level', 0) == difficulty_level:
                    if example.get('problem', '') != question1:
                        question2 = example.get('problem', '')
                        answer2 = example.get('solution', '')
                        break
            
            # Fallback
            if not question1 and similar_examples:
                question1 = similar_examples[0][2].get('problem', '')
                answer1 = similar_examples[0][2].get('solution', '')
            if not question2:
                for score, rank_idx, example in similar_examples:
                    if example.get('problem', '') != question1:
                        question2 = example.get('problem', '')
                        answer2 = example.get('solution', '')
                        break
            
            messages = [
                {"role": "system", "content": "You are a helpful math assistant. Solve problems step by step and provide your final answer in the end of the solution."},
                {"role": "user", "content": f"{question1}\n\nPlease solve this problem step by step and provide your final answer."},
                {"role": "assistant", "content": answer1},
                {"role": "user", "content": f"{question2}\n\nPlease solve this problem step by step and provide your final answer."},
                {"role": "assistant", "content": answer2},
                {"role": "user", "content": f"{problem}\n\nPlease solve this problem step by step and provide your final answer."},
            ]
    
    # å¦‚æœä½¿ç”¨ OpenAI APIï¼Œç›´æ¥è¿”å› messages
    if return_messages:
        return messages, ground_truth, problem
    
    # å¦åˆ™ä½¿ç”¨ tokenizer æ ¼å¼åŒ–
    formatted_prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
        
    return formatted_prompt, ground_truth, problem

def load_local_math500(dataset_path, split='test'):
    """åŠ è½½æœ¬åœ° Math-500 æ•°æ®é›†"""
    file_path = os.path.join(dataset_path, f"{split}.jsonl")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æœ¬åœ°æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    data_list = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data_list.append(json.loads(line))
    
    # è½¬æ¢ä¸º HuggingFace Dataset æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
    return Dataset.from_list(data_list)


def main():
    args = parse_args()
    
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu_id)
    print(f"ğŸš€ å·²è®¾ç½® CUDA_VISIBLE_DEVICES={args.gpu_id}")
    

    
    # åŠ è½½æ•°æ®é›†
    if args.dataset == 'math500' and args.use_local_dataset:
        print(f"åŠ è½½æœ¬åœ° MATH-500 æ•°æ®é›† ({args.local_dataset_path})...")

        train_data = load_local_math500(args.local_dataset_path, split='train')
        test_data = load_local_math500(args.local_dataset_path, split='test')
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
        
        if args.sample_test_size is not None:
            sample_size = min(args.sample_test_size, len(test_data))
            sample_indices = random.sample(range(len(test_data)), sample_size)
            test_data = test_data.select(sample_indices)
            print(f"æŠ½æ ·åæµ‹è¯•é›†å¤§å°: {len(test_data)}")
        
       
        example_pool_size = min(args.example_pool_size, len(train_data))
        example_pool = train_data.select(range(example_pool_size))

    if args.use_cot == 'curriculum':
        example_pool = rank_examples_by_sparsity(example_pool, args.model_path, metric=args.rank_metric)
        example_pool, difficulty_thresholds = analyze_difficulty_levels(example_pool, n_levels=args.n_levels, verbose=True)
        test_data = rank_examples_by_sparsity(test_data, args.model_path, metric=args.rank_metric)
        
        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print(f"æ˜¾å­˜å·²æ¸…ç†ï¼Œå½“å‰å¯ç”¨: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GiB")
    else:
        difficulty_thresholds = None
        
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    print("\nåŠ è½½ vLLM æ¨¡å‹...")
    llm = LLM(
        model=args.model_path,
        tensor_parallel_size=1,
        dtype=torch.float16,
        gpu_memory_utilization=args.gpu_memory_utilization,
        trust_remote_code=True,
        max_model_len=args.max_tokens
    )
    print("æ¨¡å‹åŠ è½½å®Œæˆ\n")
    sampling_params = SamplingParams(
        temperature=0.0,
        top_p=1.0,
        max_tokens=args.max_tokens
    )    

    
   
        
        
    # å‡†å¤‡æ‰€æœ‰ prompts
    print("å‡†å¤‡ prompts...")
    prompts = []
    ground_truths = []
    problems = []
    
    for i in range(len(test_data)):
        item = test_data[i]
        
        formatted_prompt, ground_truth, problem = prepare_prompt_math500(
            item, tokenizer, use_cot=args.use_cot, example_pool=example_pool, 
            num_few_shot=args.num_few_shot, dataset_type=args.dataset, model_path=args.model_path, 
            rank_metric=args.rank_metric, difficulty_thresholds=difficulty_thresholds)

        prompts.append(formatted_prompt)
        
        ground_truths.append(ground_truth)
        problems.append(problem)
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
        if i == 0:
            print("\n" + "="*80)
            print("æ ·æœ¬ç¤ºä¾‹ï¼ˆç¬¬1ä¸ªï¼‰:")
            print("="*80)
            print(f"é—®é¢˜: {problem[:200]}...")
            print(f"ç­”æ¡ˆ: {ground_truth}")
            print("="*80 + "\n")
    
    print(f"å‡†å¤‡å®Œæˆï¼Œå…± {len(test_data)} ä¸ªæ ·æœ¬\n")
    
    # æ‰¹é‡æ¨ç†
    print("å¼€å§‹æ‰¹é‡æ¨ç†...")
    outputs = llm.generate(prompts, sampling_params)
    print("æ¨ç†å®Œæˆ\n")
    
    # æå–ç”Ÿæˆçš„æ–‡æœ¬
    generated_texts = [output.outputs[0].text.strip() for output in outputs]
    
    # å¤„ç†ç»“æœ
    print("="*80)
    print("å¤„ç†ç»“æœ")
    print("="*80)
    
    correct = 0
    total = 0
    
    for i, generated_text in enumerate(generated_texts):
        ground_truth = ground_truths[i]
        
        # æå–ç­”æ¡ˆï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        predicted_answer = get_answer(generated_text)
        
        # æ£€æŸ¥æ­£ç¡®æ€§ï¼ˆä½¿ç”¨ is_equiv æ¯”è¾ƒå®Œæ•´æ–‡æœ¬ï¼‰
        is_correct = is_equiv(generated_text, ground_truth)
        
        if is_correct:
            correct += 1
        total += 1
        
        status = "âœ“" if is_correct else "âœ—"
        
        # æ ¹æ®verboseå’Œprint_freqæ‰“å°è¯¦ç»†ä¿¡æ¯
        should_print = args.verbose or (i + 1) % args.print_freq == 0 or i == 0
        
        if should_print:
            print(f"\n{'='*80}")
            print(f"[æ ·æœ¬ {i+1}/{len(test_data)}]")
            print(f"{'='*80}")
            print(f"é—®é¢˜: {problems[i]}")
            print(f"\næ¨¡å‹å›å¤:\n{generated_text}")
            print(f"\né¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
            print(f"æ­£ç¡®ç­”æ¡ˆ: {ground_truth}")
            print(f"åˆ¤æ–­ç»“æœ: {status} {'âœ“ æ­£ç¡®' if is_correct else 'âœ— é”™è¯¯'}")
            print(f"å½“å‰å‡†ç¡®ç‡: {correct/total:.2%} ({correct}/{total})")
            print(f"{'='*80}")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_accuracy = correct / total if total > 0 else 0
    
    print("\n" + "="*80)
    print("è¯„ä¼°å®Œæˆï¼")
    print("="*80)
    print(f"æ€»æ ·æœ¬æ•°: {total}")
    print(f"æ­£ç¡®æ•°: {correct}")
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.2%}")
    print("="*80)

if __name__ == "__main__":
    main()
